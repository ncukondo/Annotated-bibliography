---
bibliography: citations.bib
---

# Implementation Science

## Why?

- In health professional education, there have been many calls to improve the translation of evidence into practice[@Thomas2021-yk]
- approaches the challenge of translating evidence into practice[@Thomas2021-yk]

## How?

### 4 steps(in AL Last Page)

1. identify research–practice gaps
2. identify facilitators and barriers to the uptake of new knowledge/evidence
3. design interventions to promote uptake
4. implement and evaluate impact

[@Thomas2016-jx]

#### 1. Identifying research–practice gaps

- **Purpose**
  - Describe current practice
  - Identify best practices from best available evidence
  - Identify the nature and magnitude of research–practice gaps
  - Adapt evidence to intended audience and local context

- **Methods**
  - Knowledge syntheses
  - Portfolios
  - Surveys
  - Guided interviews
  - Focus groups
  - Systems and accreditation document reviews

- **Outcome**
  - List of important gaps
  - List of current teaching, assessment, and program activities development

- **Example**
  - Review evidence on strategies for giving residents effective feedback
  - Identify current feedback practices in residency training programs using questionnaires and focus groups
  - Inform practice and nature of the gap between current feedback practices and best practice strategies

#### 2. Identifying facilitators and barriers

- **Purpose**
  - Identify level of the facilitator/barrier:
    - **Individual**: knowledge, attitudes, motivation, skills, etc.
    - **Organizational**: availability of resources, culture, readiness to change, etc.
    - **System**: health care reforms, regulations and laws, etc.
  - Identify theoretical framework to explain reasons for the gaps:
    - Theoretical Domains Framework (TDF), Consolidated Framework for Implementation Research, etc.

- **Methods**
  - Use theories to identify and understand facilitators and barriers: motivational, social-cognitive, action theories, etc.
  - Data sources:
    - Qualitative (interviews, focus groups)
    - Quantitative (surveys)
    - Mixed approaches

- **Outcome**
  - List of facilitators and barriers with explanatory components
  - Data to inform the design of targeted strategies to improve educational practices and policies

- **Example**
  - Interviews among clinical teachers underpinned by the TDF to identify the individual and organizational supports (e.g., readiness to change; resources to support uptake of new residency training program with practice; protected time to read and discuss evidence on feedback) and barriers (e.g., lack of knowledge on effective feedback strategies; heavy patient caseloads) to effective feedback practices

#### 3. Designing interventions

- **Purpose**
  - Design interventions that are:
    - Theory-based and aligned with facilitators and barriers
    - Targeted to appropriate audience
    - Contextualized to local learning environment
    - Feasible, acceptable, sustainable
    - Developed and implemented in partnership with relevant stakeholders

- **Methods**
  - Select intervention components:
    - Map practice change techniques to facilitators and barriers (modelling, self-monitoring, graded task, skill rehearsal, etc.)
    - Evidence supporting the effect of the intervention
  - The intervention toolkit:
    - Individual (feedback, outreach visits, faculty development)
    - Organizational
    - System
  - Operationalize the intervention (targeted to whom, why, when, where, what, how often, and by whom)
  - Select mode of delivery (must be feasible, acceptable, guided by local context)

- **Outcome**
  - Theory-based tailored intervention ready for implementation

- **Example**
  - Consider who needs to do what differently, why, when, and how?
  - Involve teachers, department chairs, and residents in designing the KT interventions to promote uptake of new feedback strategies
  - For example, intervention (feedback) mapped to previously identified barrier (a specific knowledge gap) delivered online by weekly over four months by (supervisory clinician) to (a new group of residents)

#### 4. Implementing and evaluating impact

- **Purpose**
  - HPE researchers, implementation scientists, and other stakeholders evaluate intervention outcomes at three levels:
    - **Individual**: learners, teachers, etc.
    - **Organizational**: school, hospital ward, etc.
    - **System**: education, health, etc.

- **Methods**
  - Pre–post studies
  - Quasi-experimental trials
  - Controlled trials
  - Case studies
  - Cohort studies
  - Mixed methods

- **Outcome**
  - Individual outcomes
  - Organizational outcomes
  - System outcomes

- **Example**
  - Measurable changes in:
    - Knowledge, attitudes, skills, and behaviors regarding effective feedback strategies in residency training programs
    - Cost-effective and streamlined residency programs, improved learner outcomes, etc.
    - Accreditation, licensure, quality of care, safety, etc.


| Step | Purpose | Methods | Outcome | Example |
|------|---------|---------|---------|---------|
| 1. Identifying research–practice gaps | Describe current practice, identify best practices, nature and magnitude of gaps, adapt evidence to audience and context | Knowledge syntheses, portfolios, surveys, guided interviews, focus groups, document reviews | List of gaps, current activities | Review strategies for feedback, identify current practices, inform practice and nature of gaps |
| 2. Identifying facilitators and barriers | Identify level of the facilitator/barrier (individual, organizational, system), theoretical framework for gaps | Theories to identify and understand facilitators and barriers, qualitative and quantitative data sources, mixed approaches | List of facilitators and barriers, data for strategy design | Interviews with clinical teachers, identify supports and barriers |
| 3. Designing interventions | Theory-based, aligned with facilitators and barriers, targeted, contextualized, feasible, acceptable, sustainable | Select intervention components, map change techniques, evidence for effect, intervention toolkit, operationalize the intervention | Theory-based tailored intervention ready for implementation | Consider different needs, involve stakeholders, design KT interventions |
| 4. Implementing and evaluating impact | Evaluate intervention outcomes at individual, organizational, system levels | Pre–post studies, quasi-experimental trials, controlled trials, case studies, cohort studies, mixed methods | Individual, organizational, system outcomes | Measure changes in knowledge, attitudes, skills, behaviors, cost-effectiveness, accreditation, quality of care, safety |




[@Thomas2016-jx]

## References
